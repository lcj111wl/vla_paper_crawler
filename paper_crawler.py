#!/usr/bin/env python3
"""
è‡ªåŠ¨çˆ¬å– VLA (Vision-Language-Action) é¢†åŸŸæœ€æ–°è®ºæ–‡å¹¶å†™å…¥ Notion æ•°æ®åº“
æ”¯æŒæ•°æ®æºï¼šarXiv API, Semantic Scholar API
å®šæ—¶è¿è¡Œï¼šæ¯ 3 å¤©æ‰§è¡Œä¸€æ¬¡
"""

import os
import sys
import json
import time
import logging
import requests
import math
import tempfile
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from pathlib import Path

# VLA è¿‡æ»¤æ¨¡å—
from vla_filter import is_vla_related

# PDF è§£æ
try:
    import fitz  # PyMuPDF
    PDF_PARSING_AVAILABLE = True
except ImportError:
    PDF_PARSING_AVAILABLE = False
    fitz = None

# å¯¼å…¥å›¾ç‰‡æå–å™¨
try:
    from figure_extractor import FigureExtractor
    FIGURE_EXTRACTION_AVAILABLE = True
except ImportError:
    FIGURE_EXTRACTION_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("å›¾ç‰‡æå–æ¨¡å—ä¸å¯ç”¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('paper_crawler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# ===================== ç¼ºå¤±å­—æ®µè¡¥å…¨è¾…åŠ©å‡½æ•° =====================

def _derive_pdf_link(paper: Dict[str, Any]) -> Optional[str]:
    """ä» DOI æˆ– URL æ¨å¯¼ PDF é“¾æ¥

    Args:
        paper: è®ºæ–‡æ•°æ®å­—å…¸

    Returns:
        PDF é“¾æ¥æˆ– None
    """
    # å¦‚æœå·²æœ‰ PDF Linkï¼Œè¿”å› None
    if paper.get('pdf_url'):
        return None

    # 1. å°è¯•ä» arXiv ID æ„å»º
    doi = paper.get('doi', '')
    if doi.lower().startswith('arxiv:'):
        arxiv_id = doi.split(':', 1)[1]
        return f"https://arxiv.org/pdf/{arxiv_id}.pdf"

    # 2. ä» URL æ¨å¯¼ï¼ˆå¦‚æœæ˜¯ arxiv ç½‘å€ï¼‰
    url = paper.get('url', '')
    if 'arxiv.org' in url:
        if '/abs/' in url:
            arxiv_id = url.split('/abs/')[-1].split('v')[0]  # ç§»é™¤ç‰ˆæœ¬å·
            return f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        elif '/pdf/' in url:
            return url  # å·²ç»æ˜¯ PDF é“¾æ¥

    return None


def _fetch_institutions_from_semantic_scholar(paper: Dict[str, Any],
                                               ss_api_base: str = "https://api.semanticscholar.org/graph/v1") -> List[str]:
    """ä» Semantic Scholar æŸ¥è¯¢ä½œè€…æœºæ„ï¼ˆå‘è¡¨è®ºæ–‡çš„å­¦æ ¡/ä¼ä¸šç­‰ï¼‰

    Args:
        paper: è®ºæ–‡æ•°æ®å­—å…¸
        ss_api_base: Semantic Scholar API åŸºç¡€ URL

    Returns:
        æœºæ„åç§°åˆ—è¡¨ï¼ˆå¦‚ MIT, Stanford, Google DeepMind ç­‰ï¼‰
    """
    institutions = []

    try:
        # ä¼˜å…ˆçº§1: é€šè¿‡ DOI æŸ¥è¯¢ï¼ˆæœ€å‡†ç¡®ï¼‰
        doi = paper.get('doi', '')
        paper_id = None

        if doi and doi.startswith('10.'):
            paper_id = f"DOI:{doi}"
            logger.debug(f"ä½¿ç”¨ DOI æŸ¥è¯¢æœºæ„: {paper_id}")
        elif doi and doi.lower().startswith('arxiv:'):
            arxiv_id = doi.split(':', 1)[1]
            paper_id = f"arXiv:{arxiv_id}"
            logger.debug(f"ä½¿ç”¨ arXiv ID æŸ¥è¯¢æœºæ„: {paper_id}")

        # ä¼˜å…ˆçº§2: é€šè¿‡ URL æå– DOI/arXiv
        if not paper_id:
            url = paper.get('url', '')
            if 'arxiv.org' in url:
                if '/abs/' in url:
                    arxiv_id = url.split('/abs/')[-1].split('v')[0]
                    paper_id = f"arXiv:{arxiv_id}"
                    logger.debug(f"ä» URL æå– arXiv ID: {paper_id}")
            elif 'doi.org' in url:
                import re
                match = re.search(r'doi\.org/(10\.\S+)', url)
                if match:
                    paper_id = f"DOI:{match.group(1)}"
                    logger.debug(f"ä» URL æå– DOI: {paper_id}")

        # ä¼˜å…ˆçº§3: é€šè¿‡æ ‡é¢˜æœç´¢ï¼ˆæœ€ä¸å‡†ç¡®ï¼‰
        if not paper_id:
            title = paper.get('title')
            if not title:
                logger.warning(f"è®ºæ–‡ç¼ºå°‘ DOI å’Œæ ‡é¢˜ï¼Œæ— æ³•æŸ¥è¯¢æœºæ„")
                return institutions

            logger.debug(f"ä½¿ç”¨æ ‡é¢˜æœç´¢æœºæ„: {title[:50]}...")
            search_url = f"{ss_api_base}/paper/search"
            params = {"query": title, "limit": 1, "fields": "paperId"}
            response = requests.get(search_url, params=params, timeout=20)

            if response.status_code == 429:
                logger.warning("Semantic Scholar API é™æµï¼Œè·³è¿‡æœºæ„æŸ¥è¯¢")
                return institutions

            response.raise_for_status()
            data = response.json()

            if data.get('data'):
                paper_id = data['data'][0].get('paperId')
                logger.debug(f"æœç´¢åˆ°è®ºæ–‡ ID: {paper_id}")
            else:
                logger.warning(f"æœªæ‰¾åˆ°è®ºæ–‡: {title[:50]}")
                return institutions

        # æŸ¥è¯¢è®ºæ–‡è¯¦æƒ…ï¼ˆåŒ…å«ä½œè€…åŠå…¶æœºæ„ï¼‰
        paper_url = f"{ss_api_base}/paper/{paper_id}"
        params = {"fields": "authors.affiliations,authors.name"}

        response = requests.get(paper_url, params=params, timeout=20)

        if response.status_code == 429:
            logger.warning("Semantic Scholar API é™æµ")
            return institutions

        if response.status_code == 404:
            logger.warning(f"è®ºæ–‡ä¸å­˜åœ¨: {paper_id}")
            return institutions

        response.raise_for_status()
        paper_data = response.json()

        # æå–ä½œè€…æœºæ„
        authors = paper_data.get('authors', [])
        logger.info(f"ğŸ“š è®ºæ–‡æœ‰ {len(authors)} ä½ä½œè€…")

        for idx, author in enumerate(authors[:15]):  # é™åˆ¶å‰ 15 ä½ä½œè€…
            author_name = author.get('name', 'Unknown')
            affiliations = author.get('affiliations', [])

            if not affiliations:
                logger.debug(f"  ä½œè€… {idx+1}/{len(authors)}: {author_name} - æ— æœºæ„ä¿¡æ¯")
                continue

            logger.debug(f"  ä½œè€… {idx+1}/{len(authors)}: {author_name} - {len(affiliations)} ä¸ªæœºæ„")

            for aff in affiliations:
                # affiliations å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸
                if isinstance(aff, str):
                    name = aff
                elif isinstance(aff, dict):
                    name = aff.get('name') or aff.get('displayName')
                else:
                    continue

                if name and name not in institutions:
                    institutions.append(name)
                    logger.debug(f"    âœ“ æ·»åŠ æœºæ„: {name}")

                if len(institutions) >= 15:
                    break

            if len(institutions) >= 15:
                break

        if institutions:
            logger.info(f"âœ… æ‰¾åˆ° {len(institutions)} ä¸ªæœºæ„: {', '.join(institutions[:3])}...")
        else:
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æœºæ„ä¿¡æ¯")

    except Exception as e:
        logger.error(f"âŒ Semantic Scholar æœºæ„æŸ¥è¯¢å¤±è´¥: {e}")

    return institutions


def detect_missing_fields(papers: List[Dict[str, Any]],
                         check_fields: Optional[List[str]] = None) -> Dict[str, List[Dict]]:
    """æ£€æµ‹è®ºæ–‡çš„ç¼ºå¤±å­—æ®µ

    Args:
        papers: è®ºæ–‡åˆ—è¡¨ï¼ˆæ¥è‡ª fetch_existing_papersï¼‰
        check_fields: è¦æ£€æŸ¥çš„å­—æ®µåˆ—è¡¨

    Returns:
        {
            'missing_pdf_url': [{'page_id': '...', 'title': '...', ...}, ...],
            'missing_institutions': [...],
            ...
        }
    """
    if check_fields is None:
        check_fields = ['pdf_url', 'doi', 'institutions', 'citations', 'recommend_score', 'recommend_rationale']

    missing = {f'missing_{field}': [] for field in check_fields}

    for paper in papers:
        if not paper.get('page_id'):
            continue

        for field in check_fields:
            value = paper.get(field)
            is_missing = False

            # ç¼ºå¤±çš„å®šä¹‰
            if value is None:
                is_missing = True
            elif isinstance(value, str) and not value.strip():
                is_missing = True
            elif isinstance(value, list) and len(value) == 0:
                is_missing = True
            # æ³¨æ„ï¼š0 å¯¹äºæ•°å­—å­—æ®µä¸ç®—ç¼ºå¤±

            if is_missing:
                missing[f'missing_{field}'].append({
                    'page_id': paper['page_id'],
                    'title': paper.get('title', 'Unknown'),
                    'doi': paper.get('doi'),
                    'url': paper.get('url'),
                    'pdf_url': paper.get('pdf_url'),
                    'year': paper.get('year'),
                    'authors': paper.get('authors'),
                    'abstract': paper.get('abstract'),
                })

    # ç»Ÿè®¡
    stats = {k: len(v) for k, v in missing.items() if v}
    if stats:
        logger.info(f"ç¼ºå¤±å­—æ®µç»Ÿè®¡: {json.dumps(stats, ensure_ascii=False)}")

    return missing


def patch_missing_fields(notion_client: "NotionClient",
                        papers_with_missing: List[Dict],
                        field_type: str,
                        enricher: Optional["MetricsEnricher"] = None,
                        llm_engine: Optional["LLMScoringEngine"] = None,
                        max_papers: int = 10) -> Tuple[int, int]:
    """è¡¥å…¨æŒ‡å®šç±»å‹çš„ç¼ºå¤±å­—æ®µ

    Args:
        notion_client: NotionClient å®ä¾‹
        papers_with_missing: ç¼ºå¤±å­—æ®µçš„è®ºæ–‡åˆ—è¡¨
        field_type: è¦è¡¥å…¨çš„å­—æ®µç±»å‹
        enricher: MetricsEnricher å®ä¾‹ï¼ˆç”¨äº citations/institutionsï¼‰
        llm_engine: LLMScoringEngine å®ä¾‹ï¼ˆç”¨äº recommend_scoreï¼‰
        max_papers: æœ€å¤šè¡¥å…¨å¤šå°‘ç¯‡è®ºæ–‡

    Returns:
        (æˆåŠŸæ•°, å¤±è´¥æ•°)
    """
    success, failed = 0, 0
    papers_to_process = papers_with_missing[:max_papers]

    for idx, paper in enumerate(papers_to_process):
        try:
            updates = {}
            page_id = paper['page_id']

            # ä¼˜å…ˆçº§1: PDF Linkï¼ˆå¿«é€Ÿï¼Œä» arXiv/DOI æ„å»ºï¼‰
            if field_type == 'pdf_url':
                pdf_url = _derive_pdf_link(paper)
                if pdf_url:
                    updates['PDF Link'] = {'url': pdf_url}
                    logger.info(f"âœ… ç”Ÿæˆ PDF Link: {paper['title'][:40]} â†’ {pdf_url[:60]}")

            # ä¼˜å…ˆçº§2: Citationsï¼ˆSemantic Scholar APIï¼‰
            elif field_type == 'citations' and enricher:
                cites, infl_cites = enricher.enrich_semantic_scholar(paper)
                if cites is not None:
                    updates['Citations'] = {'number': int(cites)}
                    if infl_cites is not None:
                        updates['Influential Citations'] = {'number': int(infl_cites)}
                    logger.info(f"âœ… æ·»åŠ å¼•ç”¨æ•°: {paper['title'][:40]} â†’ {cites} citations")

            # ä¼˜å…ˆçº§2+: Institutionsï¼ˆSemantic Scholar APIï¼‰
            elif field_type == 'institutions':
                institutions = _fetch_institutions_from_semantic_scholar(paper)
                if institutions:
                    updates['Institutions'] = {
                        'multi_select': [{'name': inst[:100]} for inst in institutions[:15]]
                    }
                    logger.info(f"âœ… æ·»åŠ æœºæ„: {paper['title'][:40]} â†’ {len(institutions)} ä¸ªæœºæ„")

            # ä¼˜å…ˆçº§3: Recommend Scoreï¼ˆLLMï¼‰
            elif field_type == 'recommend_score' and llm_engine:
                score, rationale = llm_engine.score_paper(paper)
                if score is not None:
                    updates['Recommend Score'] = {'number': float(score)}
                    if rationale:
                        updates['Recommend Rationale'] = {
                            'rich_text': [{'text': {'content': str(rationale)[:2000]}}]
                        }
                    logger.info(f"âœ… LLM è¯„åˆ†: {paper['title'][:40]} â†’ {score}")
                    time.sleep(0.5)  # LLM API å»¶è¿Ÿ

            # æ‰§è¡Œæ›´æ–°
            if updates:
                if notion_client.update_paper_fields(page_id, updates):
                    success += 1
                else:
                    failed += 1

            time.sleep(0.3)  # Notion API é™æµä¿æŠ¤

        except Exception as e:
            logger.error(f"è¡¥å…¨å­—æ®µå¤±è´¥ ({field_type}): {paper.get('title', 'Unknown')[:40]} - {e}")
            failed += 1

    logger.info(f"ğŸ“Š {field_type} è¡¥å…¨å®Œæˆ: {success} æˆåŠŸ, {failed} å¤±è´¥")
    return success, failed


# ===================== Notion API å®¢æˆ·ç«¯ =====================


class NotionClient:
    """Notion API å®¢æˆ·ç«¯"""
    
    def __init__(self, token: str, database_id: str):
        self.token = token
        self.database_id = database_id
        self.headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28"
        }
        self.base_url = "https://api.notion.com/v1"
        self._db_properties_cache: Optional[Dict[str, Any]] = None

    def _get_database(self) -> Dict[str, Any]:
        if self._db_properties_cache is None:
            resp = requests.get(f"{self.base_url}/databases/{self.database_id}", headers=self.headers, timeout=15)
            resp.raise_for_status()
            props = resp.json().get("properties", {}) or {}
            self._db_properties_cache = props
        return self._db_properties_cache or {}

    def ensure_metrics_properties(self):
        """ç¡®ä¿æ•°æ®åº“å­˜åœ¨ç”¨äºæŒ‡æ ‡çš„å±æ€§ï¼Œå¦‚æœç¼ºå¤±åˆ™è‡ªåŠ¨åˆ›å»ºã€‚

        åˆ›å»ºçš„å±æ€§ï¼š
          - Citations: number
          - Influential Citations: number
          - Impact (2yr mean): number
        """
        desired = {
            "Citations": {"number": {}},
            "Influential Citations": {"number": {}},
            "Impact (2yr mean)": {"number": {}}
        }
        try:
            props = self._get_database()
            missing = {k: v for k, v in desired.items() if k not in props}
            if not missing:
                return
            patch_body = {"properties": missing}
            resp = requests.patch(
                f"{self.base_url}/databases/{self.database_id}",
                headers=self.headers,
                json=patch_body,
                timeout=15
            )
            resp.raise_for_status()
            # å¤±æ•ˆç¼“å­˜
            self._db_properties_cache = None
            logger.info("å·²ä¸ºæ•°æ®åº“æ·»åŠ æŒ‡æ ‡å±æ€§: %s", ", ".join(missing.keys()))
        except Exception as e:
            logger.warning("æ— æ³•è‡ªåŠ¨æ·»åŠ æŒ‡æ ‡å±æ€§ï¼ˆå¿½ç•¥ï¼Œä»å°è¯•å†™å…¥å·²å­˜åœ¨å­—æ®µï¼‰: %s", e)
    
    def ensure_enrichment_properties(self):
        """ç¡®ä¿æ•°æ®åº“å­˜åœ¨æ‰©å±•å±æ€§ï¼ˆæœºæ„ & æ¨èè¯„åˆ†ï¼‰ã€‚

        åˆ›å»ºçš„å±æ€§ï¼š
          - Institutions: multi_select
          - Recommend Score: number
          - Recommend Rationale: rich_text
        """
        desired = {
            "Institutions": {"multi_select": {}},
            "Recommend Score": {"number": {}},
            "Recommend Rationale": {"rich_text": {}}
        }
        try:
            props = self._get_database()
            missing = {k: v for k, v in desired.items() if k not in props}
            if not missing:
                return
            patch_body = {"properties": missing}
            resp = requests.patch(
                f"{self.base_url}/databases/{self.database_id}",
                headers=self.headers,
                json=patch_body,
                timeout=15
            )
            resp.raise_for_status()
            self._db_properties_cache = None
            logger.info("å·²ä¸ºæ•°æ®åº“æ·»åŠ æ‰©å±•å±æ€§: %s", ", ".join(missing.keys()))
        except Exception as e:
            logger.warning("æ— æ³•è‡ªåŠ¨æ·»åŠ æ‰©å±•å±æ€§ï¼ˆå¿½ç•¥ï¼‰: %s", e)
    
    def check_duplicate(self, title: Optional[str] = None, doi: Optional[str] = None, url: Optional[str] = None) -> bool:
        """æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²å­˜åœ¨ï¼ˆé€šè¿‡æ ‡é¢˜/DOI/URLï¼‰"""
        filters = []

        if title:
            filters.append({
                "property": "Name",
                "title": {"equals": title}
            })
        if doi:
            filters.append({
                "property": "DOI",
                "rich_text": {"equals": doi}
            })
        if url:
            filters.append({
                "property": "userDefined:URL",
                "url": {"equals": url}
            })

        if not filters:
            return False

        query_body = {
            "filter": {
                "or": filters
            }
        }

        try:
            response = requests.post(
                f"{self.base_url}/databases/{self.database_id}/query",
                headers=self.headers,
                json=query_body,
                timeout=10
            )
            response.raise_for_status()
            results = response.json().get("results", [])
            return len(results) > 0
        except Exception as e:
            logger.error(f"æ£€æŸ¥é‡å¤æ—¶å‡ºé”™: {e}")
            return False

    def filter_duplicates(self, papers: List[Dict]) -> List[Dict]:
        """æ‰¹é‡æ£€æŸ¥å¹¶è¿‡æ»¤é‡å¤è®ºæ–‡ï¼Œåœ¨æŒ‡æ ‡å¢å¼ºå’ŒLLMè¯„åˆ†ä¹‹å‰è°ƒç”¨ä»¥èŠ‚çœæˆæœ¬

        Args:
            papers: è®ºæ–‡åˆ—è¡¨

        Returns:
            å»é‡åçš„è®ºæ–‡åˆ—è¡¨
        """
        unique_papers = []
        duplicate_count = 0

        for paper in papers:
            if not self.check_duplicate(
                title=paper.get('title'),
                doi=paper.get('doi'),
                url=paper.get('url')
            ):
                unique_papers.append(paper)
            else:
                duplicate_count += 1
                logger.info(f"âŠ˜ è®ºæ–‡å·²å­˜åœ¨ï¼Œè¿‡æ»¤: {paper.get('title', 'Unknown')[:60]}")

        logger.info(f"âœ… è¿‡æ»¤å®Œæˆ: {len(unique_papers)} ç¯‡æ–°è®ºæ–‡ / {len(papers)} ç¯‡æ€»è®ºæ–‡ (è¿‡æ»¤ {duplicate_count} ç¯‡é‡å¤)")
        return unique_papers

    def fetch_existing_papers(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ä» Notion æ•°æ®åº“æŸ¥è¯¢å·²æœ‰è®ºæ–‡ä¿¡æ¯

        Args:
            limit: æ¯é¡µæŸ¥è¯¢æ•°é‡ï¼ˆNotion ä¸€æ¬¡æœ€å¤šè¿”å›100æ¡ï¼‰

        Returns:
            è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªè®ºæ–‡åŒ…å« page_id å’Œæ‰€æœ‰å…³é”®å­—æ®µ
        """
        papers = []
        has_more = True
        start_cursor = None

        # å…³é”®å­—æ®µæ˜ å°„ï¼ˆNotion â†’ Pythonï¼‰
        field_mapping = {
            'Name': 'title',
            'userDefined:URL': 'url',
            'PDF Link': 'pdf_url',
            'DOI': 'doi',
            'Year': 'year',
            'Citations': 'citations',
            'Influential Citations': 'influential_citations',
            'Institutions': 'institutions',
            'Recommend Score': 'recommend_score',
            'Recommend Rationale': 'recommend_rationale',
            'Framework Diagram': 'framework_diagram',
            'Authors': 'authors',
            'Abstract': 'abstract',
        }

        while has_more:
            try:
                query_body = {"page_size": min(limit, 100)}
                if start_cursor:
                    query_body["start_cursor"] = start_cursor

                response = requests.post(
                    f"{self.base_url}/databases/{self.database_id}/query",
                    headers=self.headers,
                    json=query_body,
                    timeout=15
                )
                response.raise_for_status()
                data = response.json()

                for page in data.get('results', []):
                    paper_dict = {'page_id': page['id']}
                    properties = page.get('properties', {})

                    # æå–å­—æ®µå€¼
                    for notion_field, py_field in field_mapping.items():
                        if notion_field not in properties:
                            paper_dict[py_field] = None
                            continue

                        prop = properties[notion_field]
                        value = None

                        # æ ¹æ®å­—æ®µç±»å‹è§£æ
                        prop_type = prop.get('type')
                        if prop_type == 'title':
                            value = ''.join([t.get('text', {}).get('content', '')
                                           for t in prop.get('title', [])])
                        elif prop_type == 'url':
                            value = prop.get('url')
                        elif prop_type == 'rich_text':
                            value = ''.join([t.get('text', {}).get('content', '')
                                           for t in prop.get('rich_text', [])])
                        elif prop_type == 'number':
                            value = prop.get('number')
                        elif prop_type == 'multi_select':
                            value = [opt.get('name') for opt in prop.get('multi_select', [])]

                        paper_dict[py_field] = value

                    papers.append(paper_dict)

                # åˆ†é¡µ
                has_more = data.get('has_more', False)
                start_cursor = data.get('next_cursor')

                if has_more:
                    logger.info(f"å·²æŸ¥è¯¢ {len(papers)} ç¯‡è®ºæ–‡ï¼ˆç»§ç»­ç¿»é¡µï¼‰...")
                    time.sleep(0.3)  # API é™æµä¿æŠ¤

            except Exception as e:
                logger.error(f"æŸ¥è¯¢å·²æœ‰è®ºæ–‡å¤±è´¥: {e}")
                break

        logger.info(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œå…± {len(papers)} ç¯‡è®ºæ–‡")
        return papers

    def update_paper_fields(self, page_id: str, updates: Dict[str, Any]) -> bool:
        """æ›´æ–°è®ºæ–‡é¡µé¢çš„å­—æ®µï¼ˆé€šç”¨PATCHæ–¹æ³•ï¼‰

        Args:
            page_id: Notion é¡µé¢ ID
            updates: å­—æ®µæ›´æ–°å­—å…¸ï¼Œæ ¼å¼å¦‚ï¼š
                    {
                        'PDF Link': {'url': 'https://...'},
                        'Citations': {'number': 100},
                        'Institutions': {'multi_select': [{'name': 'MIT'}, ...]},
                    }

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not updates:
            return True

        try:
            response = requests.patch(
                f"{self.base_url}/pages/{page_id}",
                headers=self.headers,
                json={"properties": updates},
                timeout=15
            )
            response.raise_for_status()
            return True
        except Exception as e:
            logger.error(f"æ›´æ–°é¡µé¢å­—æ®µå¤±è´¥ ({page_id[:8]}...): {e}")
            return False

    def batch_update_papers(self, updates: List[Tuple[str, Dict[str, Any]]],
                           delay_s: float = 0.3) -> int:
        """æ‰¹é‡æ›´æ–°å¤šä¸ªè®ºæ–‡é¡µé¢

        Args:
            updates: [(page_id, fields_dict), ...]
            delay_s: æ¯æ¬¡è¯·æ±‚é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰

        Returns:
            æˆåŠŸæ›´æ–°çš„æ•°é‡
        """
        success_count = 0
        for page_id, fields_dict in updates:
            if self.update_paper_fields(page_id, fields_dict):
                success_count += 1
            time.sleep(delay_s)

        logger.info(f"æ‰¹é‡æ›´æ–°å®Œæˆ: {success_count}/{len(updates)} æˆåŠŸ")
        return success_count

    def add_paper(self, paper: Dict, skip_duplicate_check: bool = False) -> Optional[str]:
        """æ·»åŠ è®ºæ–‡åˆ° Notion æ•°æ®åº“ï¼Œè¿”å›é¡µé¢IDæˆ–None

        Args:
            paper: è®ºæ–‡æ•°æ®å­—å…¸
            skip_duplicate_check: å¦‚æœä¸ºTrueï¼Œè·³è¿‡é‡å¤æ£€æŸ¥ï¼ˆå› ä¸ºå·²åœ¨æ‰¹é‡è¿‡æ»¤æ—¶æ£€æŸ¥ï¼‰

        Returns:
            æˆåŠŸæ·»åŠ æ—¶è¿”å›é¡µé¢IDï¼Œå¦åˆ™è¿”å›None
        """
        # æ£€æŸ¥é‡å¤ï¼ˆå¦‚æœæœªæå‰æ‰¹é‡è¿‡æ»¤ï¼‰
        if not skip_duplicate_check:
            if self.check_duplicate(
                title=paper.get('title'),
                doi=paper.get('doi'),
                url=paper.get('url')
            ):
                logger.info(f"è®ºæ–‡å·²å­˜åœ¨ï¼Œè·³è¿‡: {paper.get('title', 'Unknown')}")
                return None
        
        # æ„é€  Notion é¡µé¢å±æ€§
        properties = {
            "Name": {
                "title": [
                    {
                        "text": {
                            "content": paper.get('title', 'Untitled')[:2000]
                        }
                    }
                ]
            },
            "Status": {
                "select": {
                    "name": "To Read"
                }
            },
            "Venue": {
                "select": {
                    "name": paper.get('venue', 'ArXiv')
                }
            }
        }
        
        # æ·»åŠ  Added æ—¥æœŸ
        properties["Added"] = {
            "date": {
                "start": datetime.now().strftime("%Y-%m-%d")
            }
        }
        
        # æ·»åŠ å¯é€‰å­—æ®µ
        if paper.get('authors'):
            properties["Authors"] = {
                "rich_text": [
                    {"text": {"content": paper['authors'][:2000]}}
                ]
            }
        
        if paper.get('year'):
            properties["Year"] = {"number": int(paper['year'])}
        
        if paper.get('abstract'):
            properties["Abstract"] = {
                "rich_text": [
                    {"text": {"content": paper['abstract'][:2000]}}
                ]
            }
        
        if paper.get('url'):
            properties["userDefined:URL"] = {"url": paper['url']}
        
        if paper.get('pdf_url'):
            properties["PDF Link"] = {"url": paper['pdf_url']}
        
        if paper.get('doi'):
            properties["DOI"] = {
                "rich_text": [
                    {"text": {"content": paper['doi']}}
                ]
            }
        
        if paper.get('tags'):
            properties["Tags"] = {
                "multi_select": [
                    {"name": tag} for tag in paper['tags'][:10]
                ]
            }
        
        # æŒ‡æ ‡å­—æ®µï¼ˆå¯é€‰ï¼‰
        if paper.get('citations') is not None:
            properties["Citations"] = {"number": int(paper['citations'])}
        if paper.get('influential_citations') is not None:
            properties["Influential Citations"] = {"number": int(paper['influential_citations'])}
        if paper.get('impact_2yr_mean') is not None:
            try:
                properties["Impact (2yr mean)"] = {"number": float(paper['impact_2yr_mean'])}
            except Exception:
                pass

        # æœºæ„å­—æ®µï¼ˆå¦‚æœæœ‰ï¼‰
        if paper.get('institutions'):
            unique_insts = []
            for inst in paper['institutions']:
                name = inst.strip()[:100]
                if name and name not in unique_insts:
                    unique_insts.append(name)
            if unique_insts:
                properties["Institutions"] = {
                    "multi_select": [{"name": n} for n in unique_insts[:15]]
                }

        # æ¨èè¯„åˆ†å­—æ®µ
        if paper.get('recommend_score') is not None:
            try:
                properties["Recommend Score"] = {"number": float(paper['recommend_score'])}
            except Exception:
                pass

        # å¤§æ¨¡å‹è¯„åˆ†ç†ç”±ï¼ˆå¯é€‰ï¼‰
        if paper.get('recommend_rationale'):
            properties["Recommend Rationale"] = {
                "rich_text": [
                    {"text": {"content": str(paper['recommend_rationale'])[:2000]}}
                ]
            }

        page_data = {
            "parent": {"database_id": self.database_id},
            "properties": properties
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/pages",
                headers=self.headers,
                json=page_data,
                timeout=15
            )
            response.raise_for_status()
            page_id = response.json().get('id')
            logger.info(f"âœ… æˆåŠŸæ·»åŠ è®ºæ–‡: {paper.get('title', 'Unknown')}")
            return page_id
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è®ºæ–‡å¤±è´¥: {paper.get('title', 'Unknown')}, é”™è¯¯: {e}")
            return None
    
    def update_framework_diagram(self, page_id: str, image_url: str) -> bool:
        """æ›´æ–°é¡µé¢çš„Framework Diagramå­—æ®µ
        
        Args:
            page_id: Notioné¡µé¢ID
            image_url: æ¡†æ¶å›¾URL
            
        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        try:
            properties = {
                "Framework Diagram": {
                    "url": image_url
                }
            }
            
            response = requests.patch(
                f"{self.base_url}/pages/{page_id}",
                headers=self.headers,
                json={"properties": properties},
                timeout=15
            )
            response.raise_for_status()
            logger.info(f"âœ… æˆåŠŸæ›´æ–°Framework Diagram: {image_url[:60]}...")
            return True
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°Framework Diagramå¤±è´¥: {e}")
            return False

    def update_framework_image_files(self, page_id: str, image_url: str, name: str = "framework.png") -> bool:
        """æ›´æ–°é¡µé¢çš„Framework Image(æ–‡ä»¶ä¸åª’ä½“)å±æ€§ï¼Œä½¿ç”¨å¤–éƒ¨HTTPSç›´é“¾ã€‚

        Args:
            page_id: Notioné¡µé¢ID
            image_url: å…¬å¼€å¯è®¿é—®çš„å›¾ç‰‡ç›´é“¾ï¼ˆhttpsï¼Œå»ºè®®.jpg/.pngç»“å°¾ï¼‰
            name: æ˜¾ç¤ºåç§°

        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        try:
            properties = {
                "Framework Image": {
                    "files": [
                        {
                            "name": name,
                            "external": {"url": image_url}
                        }
                    ]
                }
            }

            response = requests.patch(
                f"{self.base_url}/pages/{page_id}",
                headers=self.headers,
                json={"properties": properties},
                timeout=15
            )
            response.raise_for_status()
            logger.info(f"âœ… æˆåŠŸæ›´æ–°Framework Image(files): {image_url[:60]}...")
            return True
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°Framework Image(files)å¤±è´¥: {e}")
            return False


class ArxivCrawler:
    """arXiv API çˆ¬å–å™¨"""
    
    BASE_URL = "http://export.arxiv.org/api/query"
    
    def __init__(self, keywords: List[str], days_back: int = 3):
        self.keywords = keywords
        self.days_back = days_back

    def search(self, max_results: int = 50) -> List[Dict]:
        """æœç´¢æœ€è¿‘çš„è®ºæ–‡ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
        papers: List[Dict] = []

        # æ„å»ºæœç´¢æŸ¥è¯¢ - ä½¿ç”¨ä¸¥æ ¼å…³é”®å­—
        query = 'all:"Vision-Language-Action" OR all:"VLA model" OR all:"VLA policy" OR all:"vision language action model"'

        # åˆ†é¡µå‚æ•°
        cutoff_date = datetime.now() - timedelta(days=self.days_back)
        fetched = 0
        start = 0
        page_size = 100  # å•æ¬¡è¯·æ±‚æœ€å¤§æ¡æ•°ï¼ˆé€‚åº¦ï¼Œä¸è¦è¿‡å¤§ï¼‰

        try:
            import xml.etree.ElementTree as ET
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }

            while fetched < max_results:
                remaining = max_results - fetched
                this_page = min(page_size, remaining)
                params = {
                    "search_query": query,
                    "start": start,
                    "max_results": this_page,
                    "sortBy": "submittedDate",
                    "sortOrder": "descending"
                }

                logger.info(f"æ­£åœ¨æœç´¢ arXiv: {query} (start={start}, max_results={this_page})")
                response = requests.get(self.BASE_URL, params=params, timeout=30)
                response.raise_for_status()

                root = ET.fromstring(response.content)
                entries = root.findall('atom:entry', ns)
                if not entries:
                    logger.info("arXiv æ— æ›´å¤šç»“æœï¼Œæå‰ç»“æŸåˆ†é¡µ")
                    break

                added_this_round = 0
                for entry in entries:
                    # æå–ä¿¡æ¯
                    title_elem = entry.find('atom:title', ns)
                    title = (title_elem.text or "").strip().replace('\n', ' ') if title_elem is not None else "Untitled"

                    summary_elem = entry.find('atom:summary', ns)
                    summary = (summary_elem.text or "").strip().replace('\n', ' ') if summary_elem is not None else ""

                    published_elem = entry.find('atom:published', ns)
                    published_date = (published_elem.text or "") if published_elem is not None else ""

                    # æ£€æŸ¥æ—¥æœŸï¼ˆåªè·å–æœ€è¿‘ N å¤©çš„ï¼‰
                    try:
                        pub_datetime = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
                        if pub_datetime < cutoff_date.replace(tzinfo=pub_datetime.tzinfo):
                            # å½“å‰ç»“æœå·²åˆ°è¾¾æ—¶é—´ä¸‹é™ï¼Œæœ¬é¡µåç»­æ›´æ—§ï¼Œç›´æ¥ç»“æŸå¤–å±‚å¾ªç¯
                            entries = []  # è§¦å‘å¤–å±‚ break
                            break
                    except Exception:
                        pass

                    # ä½œè€…
                    author_names = []
                    for author in entry.findall('atom:author', ns):
                        name = author.find('atom:name', ns)
                        if name is not None:
                            author_names.append(name.text)
                    authors_str = ", ".join(author_names)

                    # URL å’Œ PDF
                    url = ""
                    pdf_url = ""
                    for link in entry.findall('atom:link', ns):
                        if link.get('title') == 'pdf':
                            pdf_url = link.get('href', '')
                        else:
                            url = link.get('href', '')

                    # arXiv ID ä½œä¸º DOI æ›¿ä»£
                    arxiv_id_elem = entry.find('atom:id', ns)
                    arxiv_id_text = (arxiv_id_elem.text or "") if arxiv_id_elem is not None else ""
                    arxiv_id = arxiv_id_text.split('/')[-1] if arxiv_id_text else ""

                    # å¹´ä»½
                    year = published_date[:4] if published_date and len(published_date) >= 4 else ""

                    # ä¸¥æ ¼è¿‡æ»¤
                    if not is_vla_related(title, summary):
                        logger.debug(f"è¿‡æ»¤éVLAè®ºæ–‡: {title[:60]}")
                        continue

                    paper = {
                        'title': title,
                        'authors': authors_str,
                        'year': year,
                        'abstract': summary[:2000],
                        'url': url,
                        'pdf_url': pdf_url,
                        'doi': f"arXiv:{arxiv_id}",
                        'venue': 'ArXiv',
                        'tags': ['VLA', 'arXiv'],
                        'published_date': published_date,
                    }
                    papers.append(paper)
                    added_this_round += 1

                fetched += this_page
                start += this_page

                # å¦‚æœç”±äºæ—¶é—´ä¸‹é™è€Œæå‰ç»“æŸå½“å‰é¡µï¼Œé€€å‡ºåˆ†é¡µ
                if not entries:
                    break

                # å¦‚æœæœ¬é¡µä¸€ä¸ªéƒ½æ²¡åŠ ï¼Œå¯èƒ½å…¨éƒ¨è¢«è¿‡æ»¤ï¼Œç»§ç»­ä¸‹ä¸€é¡µï¼Œç›´åˆ°è¾¾åˆ°é™åˆ¶æˆ–æ— æ›´å¤š
                if added_this_round == 0:
                    logger.debug("æœ¬é¡µæ— æ–°å¢ï¼ˆå¯èƒ½å…¨éƒ¨è¢«è¿‡æ»¤ï¼‰ï¼Œç»§ç»­ä¸‹ä¸€é¡µ")

            logger.info(f"ä» arXiv æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼ˆåˆ†é¡µç´¯è®¡ï¼‰")
            return papers

        except Exception as e:
            logger.error(f"arXiv æœç´¢å¤±è´¥: {e}")
            return []


class SemanticScholarCrawler:
    """Semantic Scholar API çˆ¬å–å™¨ï¼ˆå¤‡ç”¨ï¼‰"""
    
    BASE_URL = "https://api.semanticscholar.org/graph/v1/paper/search"
    
    def __init__(self, keywords: List[str], days_back: int = 3, enrich_institutions: bool = False):
        self.keywords = keywords
        self.days_back = days_back
        self.enrich_institutions = enrich_institutions

    def search(self, max_results: int = 30) -> List[Dict]:
        """æœç´¢æœ€è¿‘çš„è®ºæ–‡"""
        papers = []
        
        query = " ".join(self.keywords)
        cutoff_date = (datetime.now() - timedelta(days=self.days_back)).strftime("%Y-%m-%d")
        
        params = {
            "query": query,
            "limit": max_results,
            "fields": "title,authors.name,authors.affiliations,year,abstract,url,openAccessPdf,externalIds,venue,publicationDate",
            "publicationDateOrYear": f"{cutoff_date}:"
        }
        
        try:
            logger.info(f"æ­£åœ¨æœç´¢ Semantic Scholar: {query}")
            response = requests.get(self.BASE_URL, params=params, timeout=30)
            
            # å¤„ç† 429 é™æµé”™è¯¯
            if response.status_code == 429:
                logger.warning("Semantic Scholar API é™æµ (429)ï¼Œè·³è¿‡æ­¤æ•°æ®æº")
                logger.info("æç¤º: Semantic Scholar æœ‰è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œå»ºè®®å‡å°‘æŸ¥è¯¢é¢‘ç‡æˆ–ç¨åé‡è¯•")
                return []
            
            response.raise_for_status()
            
            data = response.json()
            
            for item in data.get('data', []):
                title = item.get('title', 'Untitled')
                abstract = item.get('abstract', '')

                # ä¸¥æ ¼è¿‡æ»¤ï¼šåªä¿ç•™çœŸæ­£çš„ VLA è®ºæ–‡
                if not is_vla_related(title, abstract):
                    logger.debug(f"è¿‡æ»¤éVLAè®ºæ–‡: {title[:60]}")
                    continue
                
                authors_list = item.get('authors', [])
                authors_str = ", ".join([a.get('name', '') for a in authors_list])
                
                # è·å–å¤–éƒ¨ ID
                external_ids = item.get('externalIds', {}) or {}
                doi = external_ids.get('DOI', '')
                arxiv_id = external_ids.get('ArXiv', '')
                
                # æ„å»º PDF URLï¼ˆä¼˜å…ˆçº§ï¼šopenAccessPdf > arXiv ç›´æ¥æ„å»º > ç©ºï¼‰
                pdf_url = ""
                if item.get('openAccessPdf'):
                    pdf_url = item['openAccessPdf'].get('url', '')
                elif arxiv_id:
                    # ä» arXiv ID æ„å»º PDF é“¾æ¥
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                    logger.debug(f"ä» arXiv ID æ„å»º PDF é“¾æ¥: {arxiv_id}")
                
                # ä¿å­˜ DOIï¼ˆä¼˜å…ˆ DOIï¼Œå…¶æ¬¡ ArXiv IDï¼‰
                doi_field = doi if doi else (f"arXiv:{arxiv_id}" if arxiv_id else "")
                
                # è·å–å‘å¸ƒæ—¥æœŸï¼ˆä¼˜å…ˆä½¿ç”¨ publicationDateï¼Œå¦åˆ™ä½¿ç”¨å¹´ä»½ï¼‰
                year = item.get('year', '')
                pub_date = item.get('publicationDate', '')
                if pub_date:
                    published_date = pub_date  # æ ¼å¼å¦‚ "2024-03-15"
                elif year:
                    published_date = f"{year}-01-01"
                else:
                    published_date = ""
                
                # æœºæ„æå–ï¼ˆç›´æ¥ä»è¿”å›æ•°æ®ä¸­è·å–ï¼Œæ— éœ€é¢å¤– API è°ƒç”¨ï¼‰
                institutions: List[str] = []
                if self.enrich_institutions:
                    for a in authors_list[:20]:  # é™åˆ¶å‰ 20 ä¸ªä½œè€…
                        # ç›´æ¥ä»æœç´¢ç»“æœä¸­è·å–æœºæ„ä¿¡æ¯ï¼ˆauthors.affiliations å·²åœ¨ fields ä¸­è¯·æ±‚ï¼‰
                        affs = a.get('affiliations', []) or []
                        for aff in affs:
                            # affiliation å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸
                            if isinstance(aff, str):
                                name = aff
                            elif isinstance(aff, dict):
                                name = aff.get('name') or aff.get('displayName')
                            else:
                                continue

                            if name and name not in institutions:
                                institutions.append(name)
                                logger.debug(f"  âœ“ æ·»åŠ æœºæ„: {name}")

                        if len(institutions) >= 15:  # å®‰å…¨ä¸Šé™
                            break

                    if institutions:
                        logger.info(f"âœ… ä» {len(authors_list)} ä½ä½œè€…ä¸­æå–åˆ° {len(institutions)} ä¸ªæœºæ„")

                paper = {
                    'title': title,
                    'authors': authors_str,
                    'year': str(year),
                    'abstract': abstract[:2000],
                    'url': item.get('url', ''),
                    'pdf_url': pdf_url,
                    'doi': doi_field,  # ä¿®å¤ï¼šä½¿ç”¨ doi_field è€Œä¸æ˜¯ doi
                    'venue': item.get('venue', 'Conference'),
                    'tags': ['VLA', 'Semantic Scholar'],
                    'published_date': published_date,  # ä¿å­˜å‘å¸ƒæ—¶é—´ç”¨äºæ’åº
                    'institutions': institutions,
                }
                
                papers.append(paper)
            
            logger.info(f"ä» Semantic Scholar æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            logger.error(f"Semantic Scholar æœç´¢å¤±è´¥: {e}")
            return []


class MetricsEnricher:
    """å¼•ç”¨æ•°/å½±å“åŠ›æŒ‡æ ‡å¢å¼ºå™¨ã€‚

    - å¼•ç”¨æ•°ï¼šä¼˜å…ˆä½¿ç”¨ Semantic Scholarï¼ˆcitationCount, influentialCitationCountï¼‰
    - å½±å“å› å­è¿‘ä¼¼ï¼šä½¿ç”¨ OpenAlex Source çš„ 2yr_mean_citednessï¼ˆéå®˜æ–¹ IFï¼Œä»…ä¾›å‚è€ƒï¼‰
    """

    def __init__(self, openalex_mailto: Optional[str] = None, session: Optional[requests.Session] = None):
        self.session = session or requests.Session()
        self.ss_base_item = "https://api.semanticscholar.org/graph/v1/paper/"
        self.openalex_base = "https://api.openalex.org"
        self.mailto = openalex_mailto

    def _fetch_json(self, url: str, params: Optional[Dict[str, Any]] = None, timeout: int = 20) -> Optional[Dict[str, Any]]:
        try:
            r = self.session.get(url, params=params or {}, timeout=timeout)
            r.raise_for_status()
            return r.json()
        except Exception as e:
            logger.debug("GET %s failed: %s", url, e)
            return None

    def enrich_semantic_scholar(self, paper: Dict) -> Tuple[Optional[int], Optional[int]]:
        """è¿”å› (citations, influential_citations)"""
        fields = "citationCount,influentialCitationCount,title,venue"
        # 1) DOI
        doi = None
        if paper.get('doi'):
            d = paper['doi']
            if d.lower().startswith('doi:'):
                doi = d.split(':', 1)[1]
            elif d.lower().startswith('10.'):
                doi = d
        if doi:
            data = self._fetch_json(self.ss_base_item + f"DOI:{doi}", {"fields": fields})
            if data and 'citationCount' in data:
                return data.get('citationCount'), data.get('influentialCitationCount')
        # 2) arXiv
        if paper.get('doi', '').lower().startswith('arxiv:'):
            arx = paper['doi'].split(':', 1)[1]
            data = self._fetch_json(self.ss_base_item + f"arXiv:{arx}", {"fields": fields})
            if data and 'citationCount' in data:
                return data.get('citationCount'), data.get('influentialCitationCount')
        # 3) æ ‡é¢˜æœç´¢
        title = paper.get('title')
        if title:
            search = self._fetch_json(
                "https://api.semanticscholar.org/graph/v1/paper/search",
                {"query": title, "limit": 1, "fields": fields}
            )
            if search and search.get('data'):
                d0 = search['data'][0]
                return d0.get('citationCount'), d0.get('influentialCitationCount')
        return None, None

    def enrich_openalex(self, paper: Dict) -> Optional[float]:
        """è¿”å›è¿‘ä¼¼å½±å“æŒ‡æ ‡ï¼ˆ2yr_mean_citednessï¼‰ï¼Œéœ€è¦ä» work -> source è·å–ã€‚"""
        params = {}
        if self.mailto:
            params['mailto'] = self.mailto
        work = None
        # work by DOI or arXiv
        if paper.get('doi'):
            d = paper['doi']
            if d.lower().startswith('10.'):
                work = self._fetch_json(f"{self.openalex_base}/works/doi:{d}", params)
            elif d.lower().startswith('arxiv:'):
                work = self._fetch_json(f"{self.openalex_base}/works/arXiv:{d.split(':',1)[1]}", params)
        if work is None and paper.get('title'):
            work = self._fetch_json(f"{self.openalex_base}/works", {**params, "search": paper['title'], "per_page": 1})
            if work and isinstance(work.get('results'), list) and work['results']:
                work = work['results'][0]
        if not work:
            return None
        venue = work.get('host_venue') or {}
        source_id = venue.get('id')
        if not source_id:
            return None
        # source_id å½¢å¦‚ https://openalex.org/S123456789
        src = self._fetch_json(f"{self.openalex_base}/sources/{source_id.split('/')[-1]}", params)
        if not src:
            return None
        summary = src.get('summary_stats') or {}
        return summary.get('2yr_mean_citedness')


class ScoringEngine:
    """æ¨èè¯„åˆ†å¼•æ“ã€‚

    åŸºäºå¤šç»´åº¦å¯¹è®ºæ–‡è¿›è¡Œ 0-100 æµ®ç‚¹è¯„åˆ†ï¼š
      - æ–°é²œåº¦ (freshness)
      - å¼•ç”¨æ•° (citations)
      - å½±å“åŠ›å¼•ç”¨ (influential_citations)
      - æœŸåˆŠ/ä¼šè®®å½±å“è¿‘ä¼¼ (impact)
      - æ‘˜è¦é•¿åº¦ (abstract_length)
      - PDF å¯ç”¨æ€§ (has_pdf)
      - æ¥æºè´¨é‡ (source_quality)

    æƒé‡é€šè¿‡é…ç½® recommend_score_weights æä¾›ï¼Œç¼ºçœè‡ªåŠ¨å¡«å……ã€‚
    """
    DEFAULT_WEIGHTS = {
        "freshness": 2.0,
        "citations": 1.5,
        "influential_citations": 1.0,
        "impact": 1.0,
        "abstract_length": 0.5,
        "has_pdf": 0.5,
        "source_quality": 1.0,
    }

    def __init__(self, weights: Optional[Dict[str, float]] = None):
        self.weights = {**self.DEFAULT_WEIGHTS, **(weights or {})}

    def compute(self, paper: Dict) -> float:
        w = self.weights
        total_w = sum(w.values()) if w else 0.0
        if total_w == 0:
            return 0.0

        # Freshness: 1 - (days / 365), clipped [0,1]
        freshness = 0.0
        pub_date = paper.get('published_date') or ''
        try:
            if pub_date:
                dt = datetime.strptime(pub_date[:10], "%Y-%m-%d")
                days = (datetime.now() - dt).days
                freshness = max(0.0, 1.0 - min(days / 365.0, 1.0))
        except Exception:
            pass

        citations_val = paper.get('citations') or 0
        citations = min(1.0, (0.0 if citations_val <= 0 else (math.log10(citations_val + 1) / 3)))  # log scale
        infl_val = paper.get('influential_citations') or 0
        influential = min(1.0, (0.0 if infl_val <= 0 else (math.log10(infl_val + 1) / 2.5)))
        impact = paper.get('impact_2yr_mean') or 0.0
        impact_norm = min(1.0, impact / 5.0)  # ç²—ç•¥å½’ä¸€åŒ–
        abs_len = len(paper.get('abstract', '') or '')
        abstract_length = min(1.0, abs_len / 1500.0)
        has_pdf = 1.0 if paper.get('pdf_url') else 0.0
        source_quality = 0.8  # é»˜è®¤
        tags = paper.get('tags') or []
        if 'Semantic Scholar' in tags and 'ArXiv' not in tags:
            source_quality = 0.75
        if 'ArXiv' in tags:
            source_quality = 0.85
        # ç®€å•åŠ æƒæ±‚å’Œ
        score = (
            w["freshness"] * freshness +
            w["citations"] * citations +
            w["influential_citations"] * influential +
            w["impact"] * impact_norm +
            w["abstract_length"] * abstract_length +
            w["has_pdf"] * has_pdf +
            w["source_quality"] * source_quality
        )
        # å½’ä¸€åŒ–åˆ° 0-100
        final_score = (score / total_w) * 100.0
        return round(final_score, 2)


class PDFParser:
    """PDF å…¨æ–‡è§£æå™¨ï¼ˆæå–æ–‡æœ¬å’Œå›¾ç‰‡ç”¨äºæ·±åº¦è¯„åˆ†ï¼‰"""
    
    @staticmethod
    def extract_text_from_pdf(pdf_path: str, max_pages: int = 30, max_chars: int = 50000, extract_images: bool = True, max_images: int = 10) -> Dict[str, Any]:
        """ä»æœ¬åœ° PDF æå–æ–‡æœ¬å’Œå›¾ç‰‡ã€‚
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            max_pages: æœ€å¤šè§£æé¡µæ•°
            max_chars: æœ€å¤šæå–å­—ç¬¦æ•°
            extract_images: æ˜¯å¦æå–å›¾ç‰‡
            max_images: æœ€å¤šæå–å›¾ç‰‡æ•°ï¼ˆä¼˜å…ˆæå–å‰é¢çš„å¤§å›¾ï¼‰
        
        Returns:
            {
                "full_text": str,  # å…¨æ–‡æ–‡æœ¬
                "images": List[str],  # base64 ç¼–ç çš„å›¾ç‰‡åˆ—è¡¨
                "sections": dict,  # å„èŠ‚æ–‡æœ¬ï¼ˆå¦‚æœèƒ½è¯†åˆ«ï¼‰
                "num_pages": int,
                "num_images": int,
                "truncated": bool
            }
        """
        if not PDF_PARSING_AVAILABLE or fitz is None:
            logger.warning("PyMuPDF æœªå®‰è£…ï¼Œæ— æ³•è§£æ PDF å…¨æ–‡")
            return {"full_text": "", "images": [], "sections": {}, "num_pages": 0, "num_images": 0, "truncated": False}
        
        try:
            import base64
            from io import BytesIO
            try:
                from PIL import Image
                PIL_AVAILABLE = True
            except ImportError:
                PIL_AVAILABLE = False
                if extract_images:
                    logger.warning("PIL æœªå®‰è£…ï¼Œå°†è·³è¿‡å›¾ç‰‡å‹ç¼©")
            
            doc = fitz.open(pdf_path)
            num_pages = min(len(doc), max_pages)
            full_text = ""
            images_base64 = []
            
            # ç¬¬ä¸€éï¼šæå–æ–‡æœ¬
            for page_num in range(num_pages):
                page = doc[page_num]
                text = page.get_text()
                full_text += f"\n--- Page {page_num + 1} ---\n{text}"
                if len(full_text) >= max_chars:
                    full_text = full_text[:max_chars]
                    break
            
            # ç¬¬äºŒéï¼šæå–å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if extract_images and max_images > 0:
                image_list = []
                for page_num in range(num_pages):
                    if len(image_list) >= max_images:
                        break
                    page = doc[page_num]
                    images = page.get_images()
                    
                    for img_index, img in enumerate(images):
                        if len(image_list) >= max_images:
                            break
                        try:
                            xref = img[0]
                            base_image = doc.extract_image(xref)
                            image_bytes = base_image["image"]
                            image_size_kb = len(image_bytes) / 1024
                            
                            # è¿‡æ»¤è¿‡å°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯ logo/iconï¼‰
                            if len(image_bytes) < 2000:  # å°äº 2KBï¼ˆé™ä½é˜ˆå€¼ï¼‰
                                logger.debug(f"è·³è¿‡å°å›¾ (page {page_num+1}, {image_size_kb:.1f}KB)")
                                continue
                            
                            logger.debug(f"æ‰¾åˆ°å›¾ç‰‡ (page {page_num+1}, {image_size_kb:.1f}KB, {base_image.get('width')}x{base_image.get('height')})")
                            
                            # å‹ç¼©å¤§å›¾ç‰‡ï¼ˆé¿å…è¶…å‡º API é™åˆ¶ï¼‰
                            if PIL_AVAILABLE and len(image_bytes) > 200 * 1024:  # å¤§äº 200KB
                                try:
                                    img = Image.open(BytesIO(image_bytes))
                                    
                                    # è°ƒæ•´å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼Œæœ€å¤§è¾¹é•¿ 1024pxï¼‰
                                    max_size = 1024
                                    if max(img.size) > max_size:
                                        ratio = max_size / max(img.size)
                                        new_size = tuple(int(dim * ratio) for dim in img.size)
                                        img = img.resize(new_size, Image.LANCZOS)
                                    
                                    # è½¬ä¸º JPEG å¹¶å‹ç¼©
                                    output = BytesIO()
                                    if img.mode in ('RGBA', 'LA', 'P'):
                                        img = img.convert('RGB')
                                    img.save(output, format='JPEG', quality=85, optimize=True)
                                    image_bytes = output.getvalue()
                                    image_ext = "jpeg"
                                    logger.debug(f"å›¾ç‰‡å·²å‹ç¼©: {len(image_bytes) / 1024:.1f} KB")
                                except Exception as e:
                                    logger.debug(f"å›¾ç‰‡å‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨åŸå›¾: {e}")
                            
                            # è½¬ä¸º base64
                            image_b64 = base64.b64encode(image_bytes).decode('utf-8')
                            
                            # æ„é€  data URL æ ¼å¼ï¼ˆé€‚ç”¨äºå¤§å¤šæ•°å¤šæ¨¡æ€ APIï¼‰
                            mime_type = f"image/{image_ext}"
                            data_url = f"data:{mime_type};base64,{image_b64}"
                            
                            image_list.append({
                                "page": page_num + 1,
                                "index": img_index,
                                "data_url": data_url,
                                "size": len(image_bytes)
                            })
                            
                        except Exception as e:
                            logger.debug(f"æå–å›¾ç‰‡å¤±è´¥ (page {page_num+1}, img {img_index}): {e}")
                            continue
                
                # æŒ‰å›¾ç‰‡å¤§å°æ’åºï¼ˆå¤§å›¾æ›´é‡è¦ï¼‰
                image_list.sort(key=lambda x: x["size"], reverse=True)
                images_base64 = [img["data_url"] for img in image_list[:max_images]]
                logger.info(f"PDF æå–äº† {len(images_base64)} å¼ å›¾ç‰‡ï¼ˆå…±æ‰«æ {num_pages} é¡µï¼‰")
            
            doc.close()
            return {
                "full_text": full_text,
                "images": images_base64,
                "sections": {},
                "num_pages": num_pages,
                "num_images": len(images_base64),
                "truncated": len(full_text) >= max_chars
            }
        except Exception as e:
            logger.warning(f"PDF è§£æå¤±è´¥: {e}")
            return {"full_text": "", "images": [], "sections": {}, "num_pages": 0, "num_images": 0, "truncated": False}
    
    @staticmethod
    def download_and_parse_pdf(pdf_url: str, max_pages: int = 30, max_chars: int = 50000, extract_images: bool = True, max_images: int = 10) -> Dict[str, Any]:
        """ä¸‹è½½å¹¶è§£æ PDFï¼ˆåŒ…å«å›¾ç‰‡ï¼‰"""
        try:
            with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
                resp = requests.get(pdf_url, timeout=60, stream=True)
                resp.raise_for_status()
                for chunk in resp.iter_content(chunk_size=8192):
                    tmp.write(chunk)
                tmp_path = tmp.name
            
            result = PDFParser.extract_text_from_pdf(tmp_path, max_pages, max_chars, extract_images, max_images)
            os.unlink(tmp_path)
            return result
        except Exception as e:
            logger.warning(f"PDF ä¸‹è½½/è§£æå¤±è´¥ ({pdf_url}): {e}")
            return {"full_text": "", "images": [], "sections": {}, "num_pages": 0, "num_images": 0, "truncated": False}


class LLMScoringEngine:
    """ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ¨èè¯„åˆ†ï¼ˆæ”¯æŒ PDF å…¨æ–‡è¾“å…¥ï¼‰ã€‚

    é»˜è®¤å¯¹æ¥ OpenAI å…¼å®¹çš„ Chat Completions æ¥å£ï¼›
    - provider: "openai" æˆ– "openai-compatible"
    - api_base: é»˜è®¤ä¸º https://api.openai.com/v1
    - api_key: å»ºè®®ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY æä¾›
    - use_full_pdf: æ˜¯å¦ä¸‹è½½å¹¶è§£æ PDF å…¨æ–‡ï¼ˆé»˜è®¤ Falseï¼‰
    """
    def __init__(self, provider: str = "openai", api_key: Optional[str] = None, model: str = "gpt-4o-mini",
                 api_base: Optional[str] = None, temperature: float = 0.2, timeout: int = 30, max_tokens: int = 300,
                 use_full_pdf: bool = False, pdf_max_pages: int = 30, pdf_max_chars: int = 50000,
                 pdf_extract_images: bool = True, pdf_max_images: int = 10):
        self.provider = provider
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        self.model = model
        self.api_base = api_base.rstrip("/") if api_base else "https://api.openai.com/v1"
        self.temperature = float(temperature)
        self.timeout = int(timeout)
        self.max_tokens = int(max_tokens)
        self.use_full_pdf = use_full_pdf
        self.pdf_max_pages = pdf_max_pages
        self.pdf_max_chars = pdf_max_chars
        self.pdf_extract_images = pdf_extract_images
        self.pdf_max_images = pdf_max_images

    def _endpoint(self) -> str:
        return f"{self.api_base}/chat/completions"

    def _build_messages(self, paper: Dict, extra_instructions: Optional[str] = None, pdf_content: Optional[str] = None, pdf_images: Optional[List[str]] = None) -> List[Dict]:
        """æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆæ”¯æŒæ–‡æœ¬+å›¾ç‰‡ï¼‰"""
        if pdf_content or pdf_images:
            sys_prompt = (
                "ä½ æ˜¯VLA(Vision-Language-Action)é¢†åŸŸèµ„æ·±è®ºæ–‡è¯„å®¡ä¸“å®¶ã€‚ä½ å·²è·å¾—è®ºæ–‡çš„**å®Œæ•´PDFå…¨æ–‡å’Œå…³é”®å›¾ç‰‡**ï¼Œè¯·æ·±åº¦é˜…è¯»å¹¶åˆ†æå›¾ç‰‡åè¿›è¡Œä¸¥æ ¼ä¸”å…·æœ‰åŒºåˆ†åº¦çš„æ‰“åˆ†(0-100)ã€‚\n\n"
                "**è¯„åˆ†æ ‡å‡†ï¼ˆæƒé‡é€’å‡ï¼‰**ï¼š\n"
                "1. VLAç›¸å…³æ€§(30%)ï¼šæ˜¯å¦ç›´æ¥æ¶‰åŠè§†è§‰-è¯­è¨€-åŠ¨ä½œèåˆ/å…·èº«æ™ºèƒ½/æœºå™¨äººæ“æ§ï¼Ÿæ³›æ³›çš„å¤šæ¨¡æ€ä¸ç®—é«˜åˆ†\n"
                "2. æ–¹æ³•åˆ›æ–°æ€§(25%)ï¼šæå‡ºæ–°æ¶æ„/è®­ç»ƒèŒƒå¼/æ•°æ®ç­–ç•¥ï¼Ÿè¿˜æ˜¯ä»…å¾®è°ƒ/ç®€å•ç»„åˆç°æœ‰æ–¹æ³•ï¼Ÿæ£€æŸ¥Methodç« èŠ‚ç»†èŠ‚å’Œæ¶æ„å›¾\n"
                "3. å®éªŒä¸¥è°¨æ€§(20%)ï¼šçœŸå®æœºå™¨äººå®éªŒ>ä»¿çœŸ>çº¯æ•°æ®é›†ï¼›å¤šåœºæ™¯å¤šä»»åŠ¡>å•åœºæ™¯ï¼›æœ‰æ¶ˆèå®éªŒæ›´ä½³ã€‚ç»†çœ‹Experimentsç« èŠ‚å’Œç»“æœå›¾è¡¨\n"
                "4. æŠ€æœ¯æ·±åº¦(15%)ï¼šè§£å†³æ ¸å¿ƒéš¾ç‚¹(é•¿horizonè§„åˆ’/sim2real/æ³›åŒ–/å®‰å…¨)?è¿˜æ˜¯æµ…å±‚åº”ç”¨ï¼Ÿæ£€æŸ¥æŠ€æœ¯ç»†èŠ‚\n"
                "5. å½±å“æ½œåŠ›(10%)ï¼šé¡¶ä¼š/é«˜å¼•/çŸ¥åæœºæ„/å¼€æºä»£ç /å¯å¤ç°æ€§é«˜ï¼Ÿ\n\n"
                "**æ‰“åˆ†åŒºé—´å‚è€ƒ**ï¼š\n"
                "- 90-100: çªç ´æ€§å·¥ä½œ(æ–°èŒƒå¼/SOTAåˆ·æ¦œ/é¡¶ä¼šoral/é«˜å¼•ç”¨/å¼€æºbenchmark)\n"
                "- 75-89: ä¼˜ç§€åˆ›æ–°(æ–¹æ³•æ–°é¢–/å®éªŒæ‰å®/çœŸå®æœºå™¨äººéªŒè¯/é¡¶ä¼šaccepted)\n"
                "- 60-74: ä¸­ç­‰è´¨é‡(æœ‰ä¸€å®šåˆ›æ–°ä½†ä¸å¤Ÿçªå‡º/ä»¿çœŸä¸ºä¸»/æ™®é€šä¼šè®®)\n"
                "- 40-59: è¾¹ç¼˜ç›¸å…³(VLAç›¸å…³æ€§å¼±/æ–¹æ³•å¹³åº¸/å®éªŒå•è–„/ArXivé¢„å°æœ¬)\n"
                "- 0-39: ä¸æ¨è(ä¸VLAå…³ç³»ä¸å¤§/çº¯ç»¼è¿°æ— æ–°æ„/å®éªŒç¼ºå¤±)\n\n"
                "**ä¸¥æ ¼è¦æ±‚**ï¼š\n"
                "- é¿å…æ‰“åˆ†é›†ä¸­åœ¨70-80ï¼Œä¸»åŠ¨æ‹‰å¼€å·®è·ï¼é¡¶çº§å·¥ä½œç»™æ»¡åˆ†ï¼Œå¹³åº¸å·¥ä½œç»™ä½åˆ†\n"
                "- **å¿…é¡»åˆ†æå›¾ç‰‡å†…å®¹**ï¼ˆæ¶æ„å›¾ã€å®éªŒç»“æœå›¾ã€å¯¹æ¯”å›¾è¡¨ç­‰ï¼‰ï¼Œå¹¶åœ¨è¯„åˆ†ä¾æ®ä¸­å¼•ç”¨\n"
                "- å¿…é¡»å¼•ç”¨PDFä¸­çš„å…·ä½“ç« èŠ‚/å®éªŒ/å›¾è¡¨æ¥æ”¯æ’‘ä½ çš„è¯„åˆ†\n"
                "- å¦‚æœPDFä¸å®Œæ•´æˆ–æ— æ³•è§£æå…³é”®å†…å®¹ï¼Œåœ¨rationaleä¸­è¯´æ˜\n"
                "- **è¯„åˆ†ç†ç”±å¿…é¡»ç”¨ä¸­æ–‡ä¹¦å†™**ï¼Œä¸è¦ä½¿ç”¨è‹±æ–‡\n\n"
                "åªè¿”å›JSONæ ¼å¼: {\"score\": æ•°å­—, \"rationale\": \"<300å­—ä¸­æ–‡è¯„åˆ†ä¾æ®ï¼Œéœ€å¼•ç”¨PDFå…·ä½“å†…å®¹å’Œå›¾ç‰‡åˆ†æ>\"}ï¼Œä¸è¦å…¶å®ƒå†…å®¹ã€‚"
            )
        else:
            sys_prompt = (
                "ä½ æ˜¯VLA(Vision-Language-Action)é¢†åŸŸèµ„æ·±è®ºæ–‡è¯„å®¡ä¸“å®¶ã€‚è¯·åŸºäºå…ƒæ•°æ®è¿›è¡Œä¸¥æ ¼ä¸”å…·æœ‰åŒºåˆ†åº¦çš„æ‰“åˆ†(0-100)ã€‚\n\n"
                "**è¯„åˆ†æ ‡å‡†ï¼ˆæƒé‡é€’å‡ï¼‰**ï¼š\n"
                "1. VLAç›¸å…³æ€§(30%)ï¼šæ˜¯å¦ç›´æ¥æ¶‰åŠè§†è§‰-è¯­è¨€-åŠ¨ä½œèåˆ/å…·èº«æ™ºèƒ½/æœºå™¨äººæ“æ§ï¼Ÿæ³›æ³›çš„å¤šæ¨¡æ€ä¸ç®—é«˜åˆ†\n"
                "2. æ–¹æ³•åˆ›æ–°æ€§(25%)ï¼šæå‡ºæ–°æ¶æ„/è®­ç»ƒèŒƒå¼/æ•°æ®ç­–ç•¥ï¼Ÿè¿˜æ˜¯ä»…å¾®è°ƒ/ç®€å•ç»„åˆç°æœ‰æ–¹æ³•ï¼Ÿ\n"
                "3. å®éªŒä¸¥è°¨æ€§(20%)ï¼šçœŸå®æœºå™¨äººå®éªŒ>ä»¿çœŸ>çº¯æ•°æ®é›†ï¼›å¤šåœºæ™¯å¤šä»»åŠ¡>å•åœºæ™¯ï¼›æœ‰æ¶ˆèå®éªŒæ›´ä½³\n"
                "4. æŠ€æœ¯æ·±åº¦(15%)ï¼šè§£å†³æ ¸å¿ƒéš¾ç‚¹(é•¿horizonè§„åˆ’/sim2real/æ³›åŒ–/å®‰å…¨)?è¿˜æ˜¯æµ…å±‚åº”ç”¨ï¼Ÿ\n"
                "5. å½±å“æ½œåŠ›(10%)ï¼šé¡¶ä¼š/é«˜å¼•/çŸ¥åæœºæ„/å¼€æºä»£ç /å¯å¤ç°æ€§é«˜ï¼Ÿ\n\n"
                "**æ‰“åˆ†åŒºé—´å‚è€ƒ**ï¼š\n"
                "- 90-100: çªç ´æ€§å·¥ä½œ(æ–°èŒƒå¼/SOTAåˆ·æ¦œ/é¡¶ä¼šoral/é«˜å¼•ç”¨/å¼€æºbenchmark)\n"
                "- 75-89: ä¼˜ç§€åˆ›æ–°(æ–¹æ³•æ–°é¢–/å®éªŒæ‰å®/çœŸå®æœºå™¨äººéªŒè¯/é¡¶ä¼šaccepted)\n"
                "- 60-74: ä¸­ç­‰è´¨é‡(æœ‰ä¸€å®šåˆ›æ–°ä½†ä¸å¤Ÿçªå‡º/ä»¿çœŸä¸ºä¸»/æ™®é€šä¼šè®®)\n"
                "- 40-59: è¾¹ç¼˜ç›¸å…³(VLAç›¸å…³æ€§å¼±/æ–¹æ³•å¹³åº¸/å®éªŒå•è–„/ArXivé¢„å°æœ¬)\n"
                "- 0-39: ä¸æ¨è(ä¸VLAå…³ç³»ä¸å¤§/çº¯ç»¼è¿°æ— æ–°æ„/å®éªŒç¼ºå¤±)\n\n"
                "**ä¸¥æ ¼è¦æ±‚**ï¼š\n"
                "- é¿å…æ‰“åˆ†é›†ä¸­åœ¨70-80ï¼Œä¸»åŠ¨æ‹‰å¼€å·®è·ï¼é¡¶çº§å·¥ä½œç»™æ»¡åˆ†ï¼Œå¹³åº¸å·¥ä½œç»™ä½åˆ†\n"
                "- **è¯„åˆ†ç†ç”±å¿…é¡»ç”¨ä¸­æ–‡ä¹¦å†™**ï¼Œä¸è¦ä½¿ç”¨è‹±æ–‡\n\n"
                "åªè¿”å›JSONæ ¼å¼: {\"score\": æ•°å­—, \"rationale\": \"<200å­—ä¸­æ–‡è¯„åˆ†ä¾æ®ï¼Œéœ€è¯´æ˜æ‰£åˆ†/åŠ åˆ†åŸå› >\"}ï¼Œä¸è¦å…¶å®ƒå†…å®¹ã€‚"
            )
        if extra_instructions:
            sys_prompt += "\n\n**ç”¨æˆ·è¡¥å……è¦æ±‚**: " + str(extra_instructions)
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
        abstract = paper.get("abstract") or ""
        institutions = paper.get("institutions") or []
        
        text_content = {
            "title": paper.get("title"),
            "abstract": abstract[:1500] if abstract else "",
            "venue": paper.get("venue"),
            "year": paper.get("year"),
            "published_date": paper.get("published_date"),
            "citations": paper.get("citations"),
            "influential_citations": paper.get("influential_citations"),
            "impact_2yr_mean": paper.get("impact_2yr_mean"),
            "has_pdf": bool(paper.get("pdf_url")),
            "tags": paper.get("tags"),
            "institutions": institutions[:5] if institutions else [],
        }
        
        if pdf_content:
            text_content["full_pdf_text"] = pdf_content
        
        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨å¤šæ¨¡æ€æ ¼å¼ï¼ˆOpenAI vision API æ ¼å¼ï¼‰
        if pdf_images:
            user_content = [
                {
                    "type": "text",
                    "text": f"**è®ºæ–‡å…ƒæ•°æ®å’Œå…¨æ–‡**:\n{json.dumps(text_content, ensure_ascii=False, indent=2)}\n\n**PDFå›¾ç‰‡**ï¼ˆå…±{len(pdf_images)}å¼ ï¼Œè¯·ä»”ç»†åˆ†æï¼‰ï¼š"
                }
            ]
            for idx, img_url in enumerate(pdf_images, 1):
                user_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": img_url,
                        "detail": "high"  # é«˜ç²¾åº¦åˆ†æå›¾ç‰‡
                    }
                })
            
            return [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": user_content}
            ]
        else:
            # çº¯æ–‡æœ¬æ¨¡å¼
            return [
                {"role": "system", "content": sys_prompt},
                {"role": "user", "content": json.dumps(text_content, ensure_ascii=False, indent=2)}
            ]

    def score_paper(self, paper: Dict, extra_instructions: Optional[str] = None) -> Tuple[Optional[float], Optional[str]]:
        if not self.api_key:
            logger.warning("LLM è¯„åˆ†å·²å¯ç”¨ä½†ç¼ºå°‘ API Keyï¼Œè·³è¿‡ LLM æ‰“åˆ†")
            return None, None
        
        # å¦‚æœå¯ç”¨ PDF å…¨æ–‡è§£æ
        pdf_content = None
        pdf_images = None
        if self.use_full_pdf and paper.get("pdf_url"):
            logger.info(f"ğŸ“„ ä¸‹è½½å¹¶è§£æ PDF (å«å›¾ç‰‡): {paper.get('title', '')[:50]}...")
            pdf_result = PDFParser.download_and_parse_pdf(
                paper["pdf_url"],
                max_pages=self.pdf_max_pages,
                max_chars=self.pdf_max_chars,
                extract_images=self.pdf_extract_images,
                max_images=self.pdf_max_images
            )
            if pdf_result.get("full_text"):
                pdf_content = pdf_result["full_text"]
                pdf_images = pdf_result.get("images") or []
                logger.info(f"âœ… PDF è§£ææˆåŠŸ ({pdf_result['num_pages']} é¡µ, {len(pdf_content)} å­—ç¬¦, {len(pdf_images)} å¼ å›¾ç‰‡)")
            else:
                logger.warning(f"âš ï¸  PDF è§£æå¤±è´¥ï¼Œå›é€€åˆ°æ‘˜è¦æ‰“åˆ†")
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            messages = self._build_messages(paper, extra_instructions, pdf_content, pdf_images)
            body = {
                "model": self.model,
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
            }
            
            # ä¼°ç®—è¾“å…¥å¤§å°
            import sys
            body_size_mb = sys.getsizeof(json.dumps(body)) / (1024 * 1024)
            logger.info(f"ğŸ¤– è°ƒç”¨å¤§æ¨¡å‹ API: {self.model}ï¼ˆè¯·æ±‚å¤§å°: {body_size_mb:.2f} MBï¼Œå›¾ç‰‡æ•°: {len(pdf_images) if pdf_images else 0}ï¼‰...")
            
            resp = requests.post(self._endpoint(), headers=headers, json=body, timeout=self.timeout)
            if resp.status_code == 429:
                logger.warning("LLM æ¥å£é™æµ(429)ï¼Œè·³è¿‡è¯¥æ¡")
                return None, None
            resp.raise_for_status()
            data = resp.json()
            content = (data.get("choices") or [{}])[0].get("message", {}).get("content", "")
            content = content.strip()
            # æœŸæœ›çº¯ JSON
            try:
                obj = json.loads(content)
                score = float(obj.get("score"))
                rationale = str(obj.get("rationale", "")).strip()
                if score < 0: score = 0.0
                if score > 100: score = 100.0
                return round(score, 2), rationale[:2000]
            except Exception:
                # å®¹é”™ï¼šç²—æš´æå– 0-100 æ•°å­—
                import re
                m = re.search(r"(\d{1,3})", content)
                if m:
                    sc = min(100.0, max(0.0, float(m.group(1))))
                    return round(sc, 2), content[:2000]
                return None, content[:2000] if content else None
        except Exception as e:
            logger.debug("LLM æ‰“åˆ†å¤±è´¥: %s", e)
            return None, None


def load_config(config_path: str = "config_lcj.json") -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("å¼€å§‹æ‰§è¡Œè®ºæ–‡çˆ¬å–ä»»åŠ¡")
    logger.info("=" * 60)

    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šé…ç½®æ–‡ä»¶
    config_path = sys.argv[1] if len(sys.argv) > 1 else "config.json"
    config = load_config(config_path)

    notion_token = config.get('notion_token')
    database_id = config.get('database_id')
    keywords = config.get('keywords', ['VLA', 'Vision Language Action', 'robot foundation model'])
    days_back = config.get('days_back', 3)
    arxiv_max_results = int(config.get('arxiv_max_results', 200))
    ss_max_results = int(config.get('semantic_scholar_max_results', 50))

    if not notion_token or not database_id:
        logger.error("é…ç½®æ–‡ä»¶ç¼ºå°‘ notion_token æˆ– database_id")
        sys.exit(1)

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    notion = NotionClient(notion_token, database_id)

    # çˆ¬å–è®ºæ–‡
    all_papers = []

    # 1. arXiv
    arxiv_crawler = ArxivCrawler(keywords, days_back)
    arxiv_papers = arxiv_crawler.search(max_results=arxiv_max_results)
    all_papers.extend(arxiv_papers)

    # 2. Semantic Scholarï¼ˆå¯é€‰ï¼‰
    if config.get('use_semantic_scholar', False):
        logger.info("ç­‰å¾… 3 ç§’åæŸ¥è¯¢ Semantic Scholar (é¿å…APIé™æµ)...")
        time.sleep(3)  # å¢åŠ å»¶è¿Ÿé¿å… 429 é”™è¯¯
        ss_crawler = SemanticScholarCrawler(keywords, days_back, enrich_institutions=config.get('enrich_institutions', True))
        ss_papers = ss_crawler.search(max_results=ss_max_results)
        if ss_papers:
            all_papers.extend(ss_papers)
            logger.info(f"ä» Semantic Scholar è·å¾— {len(ss_papers)} ç¯‡é¢å¤–è®ºæ–‡")
        else:
            logger.info("Semantic Scholar æœªè¿”å›ç»“æœï¼ˆå¯èƒ½å› é™æµæˆ–æ— æ–°è®ºæ–‡ï¼‰")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_papers.sort(key=lambda p: p.get('published_date', ''), reverse=True)

    logger.info(f"æ€»å…±æ‰¾åˆ° {len(all_papers)} ç¯‡è®ºæ–‡ï¼ˆå·²æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼‰")

    # ã€ä¼˜åŒ–ã€‘æå‰è¿‡æ»¤é‡å¤è®ºæ–‡ï¼Œé¿å…æµªè´¹ API è°ƒç”¨å’Œ LLM token
    logger.info("=" * 60)
    logger.info("å¼€å§‹è¿‡æ»¤å·²å­˜åœ¨çš„è®ºæ–‡...")
    logger.info("=" * 60)
    all_papers = notion.filter_duplicates(all_papers)

    # æŒ‡æ ‡å¢å¼ºï¼ˆå¯é€‰ï¼‰
    enrich_citations = config.get('enrich_citations', True)
    enrich_impact = config.get('enrich_impact', False)
    openalex_mailto = config.get('openalex_mailto')
    if enrich_citations or enrich_impact:
        # ç¡®ä¿æŒ‡æ ‡å±æ€§å­˜åœ¨
        try:
            notion.ensure_metrics_properties()
        except Exception as e:
            logger.warning("æ— æ³•ç¡®è®¤/åˆ›å»ºæŒ‡æ ‡å±æ€§: %s", e)
        enricher = MetricsEnricher(openalex_mailto=openalex_mailto)
        for p in all_papers:
            try:
                if enrich_citations:
                    c, ic = enricher.enrich_semantic_scholar(p)
                    if c is not None:
                        p['citations'] = c
                    if ic is not None:
                        p['influential_citations'] = ic
                if enrich_impact:
                    imp = enricher.enrich_openalex(p)
                    if imp is not None:
                        p['impact_2yr_mean'] = imp
            except Exception as e:
                logger.debug("æŒ‡æ ‡å¢å¼ºå¤±è´¥ï¼ˆå¿½ç•¥è¯¥æ¡ï¼‰: %s", e)
            time.sleep(0.2)

    # æ¨èè¯„åˆ†ï¼ˆä¾èµ–éƒ¨åˆ†å¢å¼ºåçš„æŒ‡æ ‡ï¼‰
    if config.get('recommend_score_enabled', True):
        try:
            notion.ensure_enrichment_properties()
        except Exception as e:
            logger.warning("æ— æ³•ç¡®è®¤/åˆ›å»ºæ‰©å±•å±æ€§: %s", e)

        # è§„åˆ™æ‰“åˆ†ä½œä¸ºå…œåº•
        rb_weights = config.get('recommend_score_weights', {})
        rule_engine = ScoringEngine(rb_weights)

        # å¤§æ¨¡å‹æ‰“åˆ†ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆï¼‰
        llm_enabled = bool(config.get('llm_recommend_score_enabled', False))
        llm_engine = None
        llm_max_papers = int(config.get('llm_max_papers', 50))
        llm_interval_s = float(config.get('llm_call_interval_s', 0.4))
        if llm_enabled:
            llm_engine = LLMScoringEngine(
                provider=config.get('llm_provider', 'openai'),
                api_key=config.get('llm_api_key') or os.environ.get('OPENAI_API_KEY'),
                model=config.get('llm_model', 'gpt-4o-mini'),
                api_base=config.get('llm_api_base'),
                temperature=float(config.get('llm_temperature', 0.2)),
                timeout=int(config.get('llm_timeout', 60)),
                max_tokens=int(config.get('llm_max_tokens', 500)),
                use_full_pdf=bool(config.get('llm_use_full_pdf', True)),
                pdf_max_pages=int(config.get('llm_pdf_max_pages', 30)),
                pdf_max_chars=int(config.get('llm_pdf_max_chars', 50000)),
                pdf_extract_images=bool(config.get('llm_pdf_extract_images', True)),
                pdf_max_images=int(config.get('llm_pdf_max_images', 10))
            )
            if not llm_engine.api_key:
                logger.warning('LLM è¯„åˆ†å¯ç”¨ä½†æœªæä¾› API Keyï¼Œå°†å›é€€è§„åˆ™æ‰“åˆ†')
                llm_enabled = False
            if llm_engine.use_full_pdf and not PDF_PARSING_AVAILABLE:
                logger.warning('PDF å…¨æ–‡è§£æå·²å¯ç”¨ä½† PyMuPDF æœªå®‰è£…ï¼Œå°†å›é€€åˆ°æ‘˜è¦æ‰“åˆ†')
                llm_engine.use_full_pdf = False

        for idx, p in enumerate(all_papers):
            try:
                if llm_enabled and llm_engine is not None and idx < llm_max_papers:
                    score, rationale = llm_engine.score_paper(p)
                    if score is not None:
                        p['recommend_score'] = score
                        if rationale:
                            p['recommend_rationale'] = rationale
                    else:
                        p['recommend_score'] = rule_engine.compute(p)
                    time.sleep(llm_interval_s)
                else:
                    p['recommend_score'] = rule_engine.compute(p)
            except Exception as e:
                logger.debug("æ¨èè¯„åˆ†è®¡ç®—å¤±è´¥ï¼ˆå¿½ç•¥è¯¥æ¡ï¼‰: %s", e)
    
    # åˆå§‹åŒ–å›¾ç‰‡æå–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    extract_figures = config.get('extract_figures', False)
    figure_extractor = None
    if extract_figures and FIGURE_EXTRACTION_AVAILABLE:
        # è¯»å–å›¾åºŠé…ç½®
        image_host_service = config.get('image_host_service', 'auto')
        imgur_client_id = config.get('imgur_client_id', '')
        
        figure_extractor = FigureExtractor(
            notion_token, 
            max_figures=3,
            image_host_service=image_host_service,
            imgur_client_id=imgur_client_id if imgur_client_id else None
        )
        if not figure_extractor.is_available():
            logger.warning("å›¾ç‰‡æå–ä¾èµ–æœªå®‰è£…ï¼Œè·³è¿‡å›¾ç‰‡æå–åŠŸèƒ½")
            figure_extractor = None
    
    # å†™å…¥ Notion
    added_count = 0
    max_papers_to_add = config.get('max_papers', 999)  # ä»é…ç½®è¯»å–ï¼Œé»˜è®¤999ç¯‡
    for paper in all_papers:
        # è®ºæ–‡æ•°é‡é™åˆ¶
        if added_count >= max_papers_to_add:
            logger.info(f"âœ… å·²æ·»åŠ  {max_papers_to_add} ç¯‡è®ºæ–‡ï¼Œè¾¾åˆ°é…ç½®çš„ä¸Šé™")
            break

        # ç”±äºå·²åœ¨å‰é¢æ‰¹é‡è¿‡æ»¤é‡å¤ï¼Œæ­¤å¤„è·³è¿‡é‡å¤æ£€æŸ¥ä»¥æé«˜æ€§èƒ½
        page_id = notion.add_paper(paper, skip_duplicate_check=True)
        if page_id:
            added_count += 1
            
            # æå–å¹¶æ·»åŠ æ¡†æ¶å›¾
            if figure_extractor and paper.get('pdf_url'):
                try:
                    logger.info(f"æ­£åœ¨æå–è®ºæ–‡æ¡†æ¶å›¾: {paper.get('title', 'Unknown')[:50]}")
                    # process_paperè¿”å›ç¬¬ä¸€å¼ å›¾ç‰‡çš„URL (å¯èƒ½æ˜¯http URLæˆ–data: URL)
                    framework_url = figure_extractor.process_paper(paper, page_id)
                    
                    # å¦‚æœæˆåŠŸæå–åˆ°æ¡†æ¶å›¾ï¼Œå¤„ç†ç»“æœ
                    if framework_url:
                        title = paper.get('title', 'framework')[:50]
                        # æ£€æŸ¥æ˜¯å¦æ˜¯HTTPS URL
                        if framework_url.startswith('http'):
                            notion.update_framework_diagram(page_id, framework_url)
                            notion.update_framework_image_files(page_id, framework_url, name=f"{title}.png")
                            logger.info(f"âœ… Framework Diagramå·²æ›´æ–°: {framework_url[:80]}")
                        # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„
                        elif framework_url.startswith('/'):
                            logger.info(f"ğŸ“ Frameworkå›¾ç‰‡å·²ä¿å­˜åˆ°æœ¬åœ°: {framework_url}")
                            logger.info(f"   å¯ä»¥æ‰‹åŠ¨æ‰“å¼€æ–‡ä»¶å¹¶ä¸Šä¼ åˆ°Notion: https://www.notion.so/{page_id}")
                        else:
                            logger.warning("æå–åˆ°çš„é“¾æ¥æ ¼å¼ä¸æ”¯æŒ")
                        
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡æå–å¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {e}")
        
        time.sleep(0.5)  # é¿å… API é™æµ

    # ã€æ–°å¢ã€‘è¡¥å…¨å·²æœ‰è®ºæ–‡çš„ç¼ºå¤±å­—æ®µ
    patch_config = config.get('patch_config', {})
    if patch_config.get('enabled', False):
        logger.info("=" * 60)
        logger.info("å¼€å§‹è¡¥å…¨å·²æœ‰è®ºæ–‡çš„ç¼ºå¤±å­—æ®µ...")
        logger.info("=" * 60)

        # 1. æŸ¥è¯¢å·²æœ‰è®ºæ–‡
        max_scan = patch_config.get('max_papers_to_scan', 200)
        existing_papers = notion.fetch_existing_papers(limit=max_scan)

        if not existing_papers:
            logger.info("æœªæ‰¾åˆ°å·²æœ‰è®ºæ–‡ï¼Œè·³è¿‡è¡¥å…¨")
        else:
            # 2. æ£€æµ‹ç¼ºå¤±å­—æ®µ
            fields_to_check = patch_config.get('fields_to_patch', [
                'pdf_url', 'institutions', 'citations', 'recommend_score'
            ])
            missing_by_field = detect_missing_fields(existing_papers, fields_to_check)

            # 3. åˆå§‹åŒ–å¢å¼ºå™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
            enricher = None
            need_enricher = any([
                patch_config.get('citations', {}).get('enabled', False),
                patch_config.get('institutions', {}).get('enabled', False)
            ])
            if need_enricher:
                enricher = MetricsEnricher(
                    openalex_mailto=config.get('openalex_mailto')
                )

            # 4. åˆå§‹åŒ– LLMï¼ˆå¦‚æœéœ€è¦ï¼‰
            llm_engine_for_patch = None
            if patch_config.get('recommend_score', {}).get('enabled', False):
                llm_engine_for_patch = LLMScoringEngine(
                    provider=config.get('llm_provider', 'openai'),
                    api_key=config.get('llm_api_key') or os.environ.get('OPENAI_API_KEY'),
                    model=config.get('llm_model', 'gpt-4o-mini'),
                    api_base=config.get('llm_api_base'),
                    temperature=float(config.get('llm_temperature', 0.2)),
                    timeout=int(config.get('llm_timeout', 60)),
                    max_tokens=int(config.get('llm_max_tokens', 500)),
                    use_full_pdf=patch_config.get('recommend_score', {}).get('use_full_pdf', False),
                    pdf_max_pages=int(config.get('llm_pdf_max_pages', 30)),
                    pdf_max_chars=int(config.get('llm_pdf_max_chars', 50000)),
                    pdf_extract_images=bool(config.get('llm_pdf_extract_images', True)),
                    pdf_max_images=int(config.get('llm_pdf_max_images', 10))
                )

            # 5. é€å­—æ®µè¡¥å…¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            priority_order = ['pdf_url', 'citations', 'institutions', 'recommend_score']
            total_patched = 0

            for field in priority_order:
                field_config = patch_config.get(field, {})
                if not field_config.get('enabled', False):
                    logger.debug(f"âŠ˜ {field}: æœªå¯ç”¨ï¼Œè·³è¿‡")
                    continue

                missing_key = f'missing_{field}'
                if missing_key not in missing_by_field or not missing_by_field[missing_key]:
                    logger.info(f"âŠ˜ {field}: æ— ç¼ºå¤±")
                    continue

                missing_papers = missing_by_field[missing_key]
                max_papers = field_config.get('max_papers', 10)

                logger.info(f"ğŸ”§ å¼€å§‹è¡¥å…¨ {field} ({len(missing_papers)} ç¯‡ç¼ºå¤±ï¼Œé™åˆ¶ {max_papers} ç¯‡)...")
                success, failed = patch_missing_fields(
                    notion, missing_papers, field,
                    enricher=enricher,
                    llm_engine=llm_engine_for_patch,
                    max_papers=max_papers
                )
                total_patched += success

            logger.info("=" * 60)
            logger.info(f"âœ… ç¼ºå¤±å­—æ®µè¡¥å…¨å®Œæˆï¼æ€»è¡¥å…¨ {total_patched} ä¸ªå­—æ®µ")
            logger.info("=" * 60)

    logger.info("=" * 60)
    logger.info(f"ä»»åŠ¡å®Œæˆï¼æˆåŠŸæ·»åŠ  {added_count} ç¯‡æ–°è®ºæ–‡åˆ° Notion")
    if extract_figures and added_count > 0:
        logger.info("å·²å°è¯•ä¸ºè®ºæ–‡æå–æ¡†æ¶å›¾")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
